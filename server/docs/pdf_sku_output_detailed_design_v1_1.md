# Output æ¨¡å—è¯¦ç»†è®¾è®¡

> **æ–‡æ¡£ç‰ˆæœ¬**: V1.1  
> **ä¸Šæ¸¸ä¾èµ–**: TA V1.6 Â§3.7 | BA V1.1 Â§4.4a | BRD V2.1 Â§8  
> **æ¨¡å—å®šä½**: å¯¼å‡ºå±‚ â€” JSON ç»„è£… + å›¾ç‰‡å¯¼å‡º + å¢é‡å¯¼å…¥ + å¯¹è´¦è½®è¯¢  
> **è®¾è®¡åŸåˆ™**: é¡µé¢çº§å¢é‡ï¼ˆä¸ç­‰æ•´ä»½ PDFï¼‰ã€å¹‚ç­‰å¯¼å…¥ã€ç»ˆæ€å®Œæ•´æ€§

### V1.1 ä¿®è®¢è¯´æ˜

| å˜æ›´ ID | çº§åˆ« | è¯´æ˜ | æ¥æº |
|---------|------|------|------|
| P0-2 | P0 | I5 é™çº§æ–¹æ¡ˆ + ImportAdapter ç‰ˆæœ¬é€‚é…å±‚ | Qwen3+Kimi |
| P1-O1 | P1 | Upsert è¯­ä¹‰ï¼šè·¨é¡µå±æ€§ä¿®æ­£â†’å†²æ­£å·²å¯¼å…¥æ•°æ® | Gemini |
| P1-O2 | P1 | èƒŒå‹æœºåˆ¶ï¼šIMPORT_FAILED æ¯”ä¾‹é«˜æ—¶ Pipeline é™é€Ÿ | Gemini |
| P1-O3 | P1 | /ops/recovery æ‰‹åŠ¨æ¢å¤æ¥å£ | Gemini |
| P1-O4 | P1 | å¹‚ç­‰é”®å¢åŠ  revision | Qwen3 |
| P1-O5 | P1 | 4xx é”™è¯¯ç²¾ç»†åˆ†ç±» | Qwen3 |
| P1-O6 | P1 | ImageExporter è¿”å› URI è€Œéè·¯å¾„ | Kimi |
| P1-O7 | P1 | completion_snapshot Schema å®šä¹‰ | Kimi |
| P1-O8 | P1 | ç»ˆæ€åˆ¤å®šå¹¶å‘ä¿æŠ¤ï¼ˆæ¡ä»¶ UPDATEï¼‰ | DeepSeek |
| P1-O9 | P1 | 429 é€€é¿ç­–ç•¥æ”¹ä¸º 30s/60s/120s | GLM-5 |
| P1-O10 | P1 | asyncio.create_task å¼‚å¸¸æ•è·å›è°ƒ | GLM-5 |

---

## 1. æ¨¡å—èŒè´£è¾¹ç•Œ

| èŒè´£ | è¯´æ˜ | å¯¹é½ |
|------|------|------|
| **JSON ç»„è£…** | SKU + Images + Bindings â†’ ç»“æ„åŒ– JSON | TA Â§3.7 |
| **å›¾ç‰‡å¯¼å‡º** | å›¾ç‰‡æ–‡ä»¶å†™å…¥ Storage + è·¯å¾„ç®¡ç† | TA Â§3.7 |
| **å¢é‡å¯¼å…¥** | æ¯é¡µå®Œæˆå³å¯¼å…¥ä¸‹æ¸¸ç³»ç»Ÿï¼ˆstrict/lenient æ¨¡å¼ï¼‰ | T35, V1.6:P1-14 |
| **Import Adapter** | é€‚é…ä¸‹æ¸¸ APIï¼ˆå¹‚ç­‰é”®=sku_idï¼Œ4xx åˆ†ç±»ï¼‰ | TA Â§3.7 |
| **å¯¹è´¦è½®è¯¢** | IMPORTED_ASSUMED ç¡®è®¤ + IMPORT_FAILED æ»ç•™æ£€æµ‹ | V1.6:P1-9/P1-17 |
| **Job ç»ˆæ€åˆ¤å®š** | æ‰€æœ‰æœ‰æ•ˆé¡µé¢è¾¾ç»ˆæ€ â†’ FULL_IMPORTED | V1.6:P1-17 |
| **quality_warning** | ä½åˆ†è¾¨ç‡/æ­§ä¹‰ç»‘å®šæ ‡è®°ï¼Œä¸‹æ¸¸è‡ªè¡Œå†³ç­– | T78/T84 |

### ä¾èµ–

```mermaid
graph LR
    PL[Pipeline] -->|"page completed"| OUT[Output]
    CO[Collaboration] -->|"task completed"| OUT
    OUT --> Import([ä¸‹æ¸¸å¯¼å…¥ API])
    OUT --> Storage[(StorageProvider)]
    OUT --> DB[(PostgreSQL)]
    style OUT fill:#1a1f2c,stroke:#34D399,color:#E2E8F4
```

---

## 2. ç›®å½•ç»“æ„

```
app/
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ json_generator.py           # ç»“æ„åŒ– JSON ç»„è£…
â”‚   â”œâ”€â”€ image_exporter.py           # å›¾ç‰‡å¯¼å‡º â†’ URIï¼ˆéè·¯å¾„ï¼‰  [V1.1]
â”‚   â”œâ”€â”€ path_normalizer.py          # è·¯å¾„æ‹¼æ¥è§„èŒƒåŒ–
â”‚   â”œâ”€â”€ incremental_importer.py     # é¡µé¢çº§å¢é‡å¯¼å…¥ + èƒŒå‹ + å¼‚å¸¸å›è°ƒ  [V1.1]
â”‚   â”œâ”€â”€ import_adapter.py           # ä¸‹æ¸¸ API é€‚é…ï¼ˆå¹‚ç­‰+é‡è¯•+ç‰ˆæœ¬é€‚é…å±‚ï¼‰  [V1.1]
â”‚   â”œâ”€â”€ reconciliation_poller.py    # å¯¹è´¦è½®è¯¢ + Job ç»ˆæ€åˆ¤å®š + I5 é™çº§  [V1.1]
â”‚   â”œâ”€â”€ backpressure.py             # [V1.1] èƒŒå‹ä¿¡å·ï¼šIMPORT_FAILED æ¯”ä¾‹ç›‘æ§
â”‚   â”œâ”€â”€ schemas.py                  # [V1.1] + completion_snapshot Schema
â”‚   â”œâ”€â”€ repository.py
â”‚   â””â”€â”€ constants.py
```

---

## 3. æ ¸å¿ƒæ—¶åºå›¾

### 3.1 é¡µé¢çº§å¢é‡å¯¼å…¥

```mermaid
sequenceDiagram
    autonumber
    participant PL as Pipeline
    participant IMP as IncrementalImporter
    participant JG as JSONGenerator
    participant IE as ImageExporter
    participant IA as ImportAdapter
    participant DB as PostgreSQL
    participant API as ä¸‹æ¸¸ç³»ç»Ÿ

    PL->>+IMP: on_page_completed(job, page_result)
    
    loop æ¯ä¸ª SKU
        IMP->>IMP: check sku.validity
        alt validity = full
            IMP->>JG: assemble(sku, images, bindings)
            JG-->>IMP: sku_json (å« quality_warning)
            IMP->>IE: export_images(sku, job)
            IE-->>IMP: image_paths[]
            IMP->>+IA: import_sku(sku_json, images)
            IA->>API: POST /products (idempotency_key=sku_id)
            alt 2xx ç¡®è®¤
                API-->>IA: confirmed
                IA-->>-IMP: IMPORTED_CONFIRMED
            else 2xx ä½†æœªç«‹å³ç¡®è®¤
                API-->>IA: accepted
                IA-->>IMP: IMPORTED_ASSUMED
            else 4xx å®¢æˆ·ç«¯é”™è¯¯
                IA-->>IMP: åˆ†ç±»é”™è¯¯(ä¸é‡è¯•)
            else 5xx
                IA->>IA: é‡è¯• 3 æ¬¡(æŒ‡æ•°é€€é¿)
                IA-->>IMP: IMPORT_FAILED
            end
        else validity = partial
            alt lenient æ¨¡å¼
                IMP->>IMP: æ ‡è®° quality_warning
                IMP->>IA: import_sku(...)
            else strict æ¨¡å¼(é»˜è®¤)
                IMP->>DB: åˆ›å»º HumanTask(SKU_CONFIRM)
            end
        else validity = invalid
            Note over IMP: è·³è¿‡ï¼Œä¸å¯¼å…¥
        end
    end

    IMP->>DB: UPDATE pages SET status = result_status
    IMP-->>-PL: done
```

### 3.2 å¯¹è´¦è½®è¯¢ + Job ç»ˆæ€

```mermaid
sequenceDiagram
    autonumber
    participant Cron as APScheduler(30min)
    participant RP as ReconciliationPoller
    participant IA as ImportAdapter
    participant DB as PostgreSQL

    Cron->>+RP: reconcile()

    rect rgb(30, 40, 60)
    Note over RP: 1. IMPORTED_ASSUMED ç¡®è®¤
    RP->>DB: SELECT * FROM pages WHERE status='IMPORTED_ASSUMED'
    loop æ¯ä¸ª assumed é¡µ
        RP->>IA: check_import_status(job_id, page)
        alt å·²ç¡®è®¤
            RP->>DB: UPDATE status='IMPORTED_CONFIRMED'
        end
    end
    end

    rect rgb(40, 30, 50)
    Note over RP: 2. IMPORT_FAILED æ»ç•™æ£€æµ‹(>24h)
    RP->>DB: SELECT * FROM pages<br/>WHERE status='IMPORT_FAILED'<br/>AND updated_at < now()-24h
    loop æ¯ä¸ªæ»ç•™é¡µ
        RP->>DB: UPDATE status='SKIPPED'
        RP->>RP: metrics.import_stale_skipped_total++
    end
    end

    rect rgb(30, 50, 40)
    Note over RP: 3. Job ç»ˆæ€åˆ¤å®š
    RP->>DB: SELECT jobs WHERE status IN (PROCESSING, PARTIAL_IMPORTED, DEGRADED_HUMAN)
    loop æ¯ä¸ªæ´»è·ƒ Job
        RP->>DB: æŸ¥è¯¢è¯¥ Job æ‰€æœ‰æœ‰æ•ˆé¡µé¢çŠ¶æ€
        alt å…¨éƒ¨ IMPORTED_CONFIRMED æˆ– IMPORTED_ASSUMED
            RP->>DB: UPDATE job SET status='FULL_IMPORTED'<br/>+ completion_snapshot
        end
    end
    end

    RP-->>-Cron: done
```

---

## 4. ç»„ä»¶è¯¦ç»†è§„æ ¼

### 4.1 IncrementalImporter

```python
class IncrementalImporter:
    """
    é¡µé¢çº§å¢é‡å¯¼å…¥ï¼ˆT35ï¼‰ï¼š
    - full validity â†’ ç«‹å³å¯¼å…¥
    - partial + lenient â†’ æ ‡è®° warning åå¯¼å…¥
    - partial + strict â†’ åˆ›å»ºäººå·¥ç¡®è®¤ä»»åŠ¡
    - invalid â†’ è·³è¿‡

    [V1.1] å˜æ›´ï¼š
    - P1-O1: Upsert è¯­ä¹‰ï¼ˆè·¨é¡µå±æ€§ä¿®æ­£â†’UPDATE_REQ å†²æ­£å·²å¯¼å…¥æ•°æ®ï¼‰
    - P1-O2: èƒŒå‹æ£€æŸ¥ï¼ˆIMPORT_FAILED æ¯”ä¾‹ >20% æ—¶é€šçŸ¥ Pipeline é™é€Ÿï¼‰
    - P1-O6: ImageExporter è¿”å› URI è€Œéæ–‡ä»¶è·¯å¾„ï¼ˆä¸º OSS è¿ç§»åšå‡†å¤‡ï¼‰
    - P1-O10: asyncio.create_task æ³¨å†Œå¼‚å¸¸å›è°ƒï¼ˆä¸é™é»˜ä¸¢å¤±ï¼‰
    """

    async def on_page_completed(self, job: PDFJob, result: PageResult):
        profile = config.get_profile(job.frozen_config_version)

        # [V1.1 P1-O2] èƒŒå‹æ£€æŸ¥
        if self._backpressure.is_throttled(job.job_id):
            logger.warning("import_throttled", job_id=str(job.job_id))
            await asyncio.sleep(self._backpressure.delay_seconds)

        for sku in (result.skus or []):
            if sku.validity == "full":
                task = asyncio.create_task(self._import_safe(sku, job, result))
                task.add_done_callback(self._on_import_done)  # [V1.1 P1-O10]
            elif sku.validity == "partial":
                if profile.sku_validity_mode == "lenient":
                    sku.quality_warning = "partial_sku_lenient_import"
                    task = asyncio.create_task(self._import_safe(sku, job, result))
                    task.add_done_callback(self._on_import_done)
                else:
                    await self._collab.create_task(
                        job_id=str(job.job_id), page_number=sku.page_number,
                        task_type="SKU_CONFIRM",
                        context={"sku": sku.to_dict()})
            # invalid â†’ skip

    async def _import_safe(self, sku, job, result):
        try:
            json_payload = self._json_gen.assemble(sku, result.images, result.bindings)
            # [V1.1 P1-O6] ImageExporter è¿”å› URI è€Œéè·¯å¾„
            image_uris = await self._img_export.export_to_uri(sku, job)
            import_result = await self._adapter.import_sku(
                json_payload, image_uris,
                revision=sku.revision)  # [V1.1 P1-O4]

            status = ("IMPORTED_CONFIRMED" if import_result.confirmed
                      else "IMPORTED_ASSUMED")
            await self._repo.update_page_status(sku.page_id, status)
            # [V1.1 P1-O2] èƒŒå‹ï¼šæˆåŠŸæ—¶é‡ç½®è®¡æ•°
            self._backpressure.on_success(job.job_id)
        except ImportDataError as e:
            # [V1.1 P1-O5] 4xx æ•°æ®é”™è¯¯ â†’ ä¸é‡è¯•ï¼Œç›´æ¥å¤±è´¥
            logger.error("import_data_error", sku_id=sku.sku_id, error=str(e))
            await self._repo.update_page_status(sku.page_id, "IMPORT_FAILED")
            self._backpressure.on_failure(job.job_id)
        except Exception as e:
            logger.error("import_failed", sku_id=sku.sku_id, error=str(e))
            await self._repo.update_page_status(sku.page_id, "IMPORT_FAILED")
            self._backpressure.on_failure(job.job_id)

    def _on_import_done(self, task: asyncio.Task):
        """[V1.1 P1-O10] Fire-and-forget ä»»åŠ¡å¼‚å¸¸å›è°ƒ"""
        if task.cancelled():
            return
        exc = task.exception()
        if exc:
            logger.error("import_task_unhandled", error=str(exc), exc_info=exc)
            metrics.import_unhandled_error_total.inc()

    async def on_cross_page_correction(self, job: PDFJob, corrected_attr: dict):
        """
        [V1.1 P1-O1] Upsert è¯­ä¹‰ï¼šè·¨é¡µå±æ€§ä¿®æ­£ â†’ å†²æ­£å·²å¯¼å…¥æ•°æ®

        å½“ Pipeline/Collaboration å‘ç°è·¨é¡µå…¨å±€å±æ€§ï¼ˆå¦‚å“ç‰Œ/ç³»åˆ—åï¼‰éœ€ä¿®æ­£æ—¶ï¼Œ
        å¯¹å·²å¯¼å…¥çš„ SKU å‘èµ· UPDATE_REQã€‚
        """
        affected_skus = await self._repo.get_imported_skus_by_job(job.job_id)
        for sku in affected_skus:
            if sku.needs_correction(corrected_attr):
                update_payload = sku.build_correction_payload(corrected_attr)
                await self._adapter.upsert_sku(
                    update_payload, revision=sku.revision + 1)
                await self._repo.update_sku_revision(sku.sku_id, sku.revision + 1)
                metrics.import_upsert_total.inc()
```

### 4.1.1 BackpressureMonitor â€” èƒŒå‹ä¿¡å·

```python
class BackpressureMonitor:
    """
    [V1.1 P1-O2] èƒŒå‹æœºåˆ¶ï¼šå½“ IMPORT_FAILED æ¯”ä¾‹è¶…é˜ˆå€¼æ—¶é€šçŸ¥ Pipeline é™é€Ÿ

    - ç»Ÿè®¡æœ€è¿‘ N æ¬¡å¯¼å…¥çš„å¤±è´¥æ¯”ä¾‹
    - æ¯”ä¾‹ >20% â†’ is_throttled=True â†’ Pipeline Semaphore ç¼©å‡
    """

    WINDOW_SIZE = 50
    FAILURE_THRESHOLD = 0.20
    THROTTLE_DELAY = 5.0  # ç§’

    def __init__(self):
        self._windows: dict[str, deque] = {}

    def on_success(self, job_id: str):
        self._record(job_id, True)

    def on_failure(self, job_id: str):
        self._record(job_id, False)

    def _record(self, job_id: str, success: bool):
        if job_id not in self._windows:
            self._windows[job_id] = deque(maxlen=self.WINDOW_SIZE)
        self._windows[job_id].append(success)

    def is_throttled(self, job_id: str) -> bool:
        window = self._windows.get(job_id)
        if not window or len(window) < 10:
            return False
        failure_rate = 1.0 - (sum(window) / len(window))
        return failure_rate > self.FAILURE_THRESHOLD

    @property
    def delay_seconds(self) -> float:
        return self.THROTTLE_DELAY
```

### 4.2 ImportAdapter â€” å¹‚ç­‰ + 4xx åˆ†ç±» + ç‰ˆæœ¬é€‚é…å±‚

```python
class ImportAdapter:
    """
    ä¸‹æ¸¸ API é€‚é…å±‚ï¼š

    [V1.1] å˜æ›´ï¼š
    - P0-2: ç‰ˆæœ¬é€‚é…å±‚ â€” æ”¯æŒ V1/V2 API å¹¶è¡Œï¼ŒI5 ä¸æˆç«‹æ—¶é™çº§
    - P1-O4: å¹‚ç­‰é”® = f"{sku_id}_v{revision}"ï¼ˆAI é‡è¯•ä¸è¦†ç›–æ—§ç»“æœï¼‰
    - P1-O5: 4xx ç²¾ç»†åˆ†ç±»ï¼š400â†’æ•°æ®é”™è¯¯(ä¸é‡è¯•) / 409â†’CASé‡è¯• / 429â†’é•¿é€€é¿
    - P1-O9: 429 é€€é¿æ”¹ä¸º 30s/60s/120sï¼ˆV1.0 ä»… 5s/10s/20s è¿œä¸å¤Ÿï¼‰
    """

    MAX_RETRIES = 3
    TIMEOUT = 30
    BACKOFF_429 = [30, 60, 120]          # [V1.1 P1-O9] é™æµé€€é¿
    BACKOFF_5XX = [2, 4, 8]              # æœåŠ¡ç«¯é”™è¯¯é€€é¿
    MAX_BACKOFF = 300                     # 5min ç¡¬ä¸Šé™

    def __init__(self, api_config: dict):
        self._import_url = api_config["import_url"]
        self._check_url = api_config.get("check_url")  # [P0-2] å¯èƒ½ä¸å­˜åœ¨
        self._api_version = api_config.get("api_version", "v1")
        self._i5_supported = api_config.get("i5_check_status", True)

    async def import_sku(
        self, payload: dict, image_uris: list[str], revision: int = 1
    ) -> ImportResult:
        # [V1.1 P1-O4] å¹‚ç­‰é”®åŒ…å« revision
        idempotency_key = f"{payload['sku_id']}_v{revision}"

        for attempt in range(self.MAX_RETRIES + 1):
            try:
                resp = await self._http.post(
                    self._import_url,
                    json={**payload, "image_uris": image_uris},
                    headers={
                        "Idempotency-Key": idempotency_key,
                        "X-API-Version": self._api_version,
                    },
                    timeout=self.TIMEOUT,
                )
                if resp.status_code in (200, 201):
                    return ImportResult(confirmed=True)
                elif resp.status_code == 202:
                    return ImportResult(confirmed=False)  # IMPORTED_ASSUMED
                elif resp.status_code == 409:
                    if attempt < self.MAX_RETRIES:
                        await asyncio.sleep(min(1 * (2 ** attempt), self.MAX_BACKOFF))
                        continue
                elif resp.status_code == 429:
                    # [V1.1 P1-O9] é™æµé€€é¿ 30s/60s/120s
                    delay = (self.BACKOFF_429[attempt]
                             if attempt < len(self.BACKOFF_429)
                             else self.MAX_BACKOFF)
                    await asyncio.sleep(delay)
                    continue
                elif 400 <= resp.status_code < 500:
                    raise ImportDataError(f"4xx: {resp.status_code}", resp.text)
                else:
                    if attempt < self.MAX_RETRIES:
                        delay = (self.BACKOFF_5XX[attempt]
                                 if attempt < len(self.BACKOFF_5XX)
                                 else self.MAX_BACKOFF)
                        await asyncio.sleep(delay)
                        continue
                    raise ImportServerError(f"5xx: {resp.status_code}")
            except (TimeoutError, ConnectionError) as e:
                if attempt < self.MAX_RETRIES:
                    await asyncio.sleep(self.BACKOFF_5XX[min(attempt, 2)])
                    continue
                raise

        raise ImportServerError("Max retries exceeded")

    async def upsert_sku(self, payload: dict, revision: int) -> ImportResult:
        """[V1.1 P1-O1] Upsert è¯­ä¹‰ï¼šå†²æ­£å·²å¯¼å…¥çš„ SKU å±æ€§"""
        idempotency_key = f"{payload['sku_id']}_v{revision}"
        resp = await self._http.put(
            f"{self._import_url}/{payload['sku_id']}",
            json=payload,
            headers={"Idempotency-Key": idempotency_key},
            timeout=self.TIMEOUT,
        )
        if resp.status_code in (200, 201):
            return ImportResult(confirmed=True)
        raise ImportServerError(f"Upsert failed: {resp.status_code}")

    async def check_status(self, job_id: str, page_number: int) -> bool:
        """
        [V1.1 P0-2] I5 é™çº§æ–¹æ¡ˆï¼š
        - I5 æˆç«‹ï¼šè°ƒç”¨ check_url æ‰¹é‡æŸ¥è¯¢
        - I5 ä¸æˆç«‹ï¼šè¿”å› Noneï¼ˆç”± ReconciliationPoller èµ°é™çº§é€»è¾‘ï¼‰
        """
        if not self._i5_supported or not self._check_url:
            return None  # é™çº§ä¿¡å·

        try:
            resp = await self._http.get(
                f"{self._check_url}?job_id={job_id}&page={page_number}",
                timeout=10)
            if resp.status_code == 200:
                return resp.json().get("confirmed", False)
            return None
        except Exception:
            return None
```

### 4.3 ReconciliationPoller â€” å¯¹è´¦

```python
class ReconciliationPoller:
    """
    [V1.1] å˜æ›´ï¼š
    - P0-2: I5 é™çº§ â€” check_status è¿”å› None æ—¶èµ° ASSUMED è¶…æ—¶è‡ªåŠ¨ç¡®è®¤
    - P1-O7: completion_snapshot Schema æ˜ç¡®å®šä¹‰
    - P1-O8: ç»ˆæ€åˆ¤å®šå¹¶å‘ä¿æŠ¤ï¼ˆæ¡ä»¶ UPDATE é¿å…é‡å¤è§¦å‘ï¼‰
    """

    ASSUMED_INTERVAL = 30 * 60      # 30min
    FAILED_STALE_THRESHOLD = 86400  # 24hï¼ˆå¯é€šè¿‡ Config è¦†ç›–ï¼‰
    ASSUMED_AUTO_CONFIRM_SEC = 86400  # [V1.1 P0-2] I5 é™çº§ï¼š24h è‡ªåŠ¨ç¡®è®¤

    async def reconcile(self):
        # 1. IMPORTED_ASSUMED â†’ ç¡®è®¤
        assumed = await self._repo.get_pages_by_status("IMPORTED_ASSUMED")
        for page in assumed:
            confirmed = await self._adapter.check_status(page.job_id, page.page_number)
            if confirmed is True:
                await self._repo.update_page_status(page.id, "IMPORTED_CONFIRMED")
                metrics.reconcile_confirmed_total.inc()
            elif confirmed is None:
                # [V1.1 P0-2] I5 ä¸æˆç«‹é™çº§ï¼šASSUMED è¶… 24h è‡ªåŠ¨ç¡®è®¤
                age = (datetime.utcnow() - page.updated_at).total_seconds()
                if age > self.ASSUMED_AUTO_CONFIRM_SEC:
                    await self._repo.update_page_status(
                        page.id, "IMPORTED_CONFIRMED",
                        trigger="assumed_auto_confirm_i5_degraded")
                    metrics.reconcile_auto_confirmed_total.inc()
                    logger.info("assumed_auto_confirmed",
                        page_id=page.id, age_hours=age / 3600)

        # 2. IMPORT_FAILED æ»ç•™ï¼ˆ>24hï¼‰â†’ SKIPPED
        stale = await self._repo.get_stale_failed(self.FAILED_STALE_THRESHOLD)
        for page in stale:
            await self._repo.update_page_status(page.id, "SKIPPED",
                                                 trigger="stale_import_timeout")
            metrics.import_stale_skipped_total.inc()

        # 3. Job ç»ˆæ€åˆ¤å®š
        await self._check_job_completion()

    async def _check_job_completion(self):
        active_jobs = await self._repo.get_active_jobs()
        for job in active_jobs:
            page_statuses = await self._repo.get_effective_page_statuses(job.job_id)
            terminal = {"IMPORTED_CONFIRMED", "IMPORTED_ASSUMED", "SKIPPED"}
            if all(p.status in terminal for p in page_statuses):
                snapshot = self._build_completion_snapshot(page_statuses, job)
                # [V1.1 P1-O8] å¹¶å‘ä¿æŠ¤ï¼šæ¡ä»¶ UPDATE é¿å…é‡å¤è§¦å‘
                updated = await self._repo.update_job_status_conditional(
                    job.job_id,
                    expected_status_not_in=["FULL_IMPORTED"],
                    new_status="FULL_IMPORTED",
                    extra={"completion_snapshot": snapshot})
                if updated:
                    metrics.job_full_imported_total.inc()

    def _build_completion_snapshot(self, page_statuses, job) -> dict:
        """
        [V1.1 P1-O7] completion_snapshot Schema:
        {
            "snapshot_at": "2025-02-15T12:00:00Z",
            "total_pages": 50,
            "effective_pages": 48,
            "status_distribution": {
                "IMPORTED_CONFIRMED": 45,
                "IMPORTED_ASSUMED": 2,
                "SKIPPED": 1
            },
            "assumed_count": 2,
            "skipped_count": 1,
            "confirmed_count": 45,
            "i5_degraded": false,
            "evidence": {
                "first_import_at": "...",
                "last_import_at": "...",
                "total_skus_imported": 230
            }
        }
        """
        from collections import Counter
        dist = Counter(p.status for p in page_statuses)
        return {
            "snapshot_at": datetime.utcnow().isoformat(),
            "total_pages": job.total_pages,
            "effective_pages": len(page_statuses),
            "status_distribution": dict(dist),
            "assumed_count": dist.get("IMPORTED_ASSUMED", 0),
            "skipped_count": dist.get("SKIPPED", 0),
            "confirmed_count": dist.get("IMPORTED_CONFIRMED", 0),
            "i5_degraded": not self._adapter._i5_supported,
            "evidence": {
                "first_import_at": min(
                    (p.updated_at for p in page_statuses), default=None),
                "last_import_at": max(
                    (p.updated_at for p in page_statuses), default=None),
            },
        }
```

---

## 5. Prometheus æŒ‡æ ‡

```python
sku_import_total = Counter("sku_import_total", "", ["status", "validity"])
import_duration_seconds = Histogram("import_duration_seconds", "")
import_stale_skipped_total = Counter("import_stale_skipped_total", "")
import_adapter_error_total = Counter("import_adapter_error_total", "", ["status_code"])
reconcile_confirmed_total = Counter("reconcile_confirmed_total", "")
# [V1.1] æ–°å¢
import_upsert_total = Counter("import_upsert_total", "Upsert corrections")
import_unhandled_error_total = Counter("import_unhandled_error_total", "Unhandled async task errors")
import_backpressure_throttled_total = Counter("import_backpressure_throttled_total", "")
reconcile_auto_confirmed_total = Counter("reconcile_auto_confirmed_total", "I5 degraded auto-confirm")
job_full_imported_total = Counter("job_full_imported_total", "")
```

---

## 6. äº¤ä»˜æ¸…å•

| æ–‡ä»¶ | è¡Œæ•°(ä¼°) | ä¼˜å…ˆçº§ | V1.1 å˜æ›´ |
|------|---------|--------|----------|
| `incremental_importer.py` | ~250 | P0 | +100: èƒŒå‹/å¼‚å¸¸å›è°ƒ/upsert/URI |
| `import_adapter.py` | ~200 | P0 | +80: ç‰ˆæœ¬é€‚é…å±‚/revisionå¹‚ç­‰/429é€€é¿/upsert |
| `json_generator.py` | ~100 | P0 | â€” |
| `image_exporter.py` | ~100 | P0 | +20: è¿”å› URI è€Œéè·¯å¾„ |
| `path_normalizer.py` | ~40 | P0 | â€” |
| `reconciliation_poller.py` | ~280 | P0 | +100: I5é™çº§/snapshot Schema/å¹¶å‘ä¿æŠ¤ |
| `backpressure.py` | ~60 | P1 | ğŸ†• æ–°å¢ |
| `schemas.py` | ~100 | P0 | +40: CompletionSnapshot / ImportResult |
| `repository.py` | ~100 | P0 | +20: update_job_status_conditional |
| `constants.py` | ~30 | P0 | +10: æ–°å¢é…ç½®é¡¹ |
| **æ€»è®¡** | **~1260** | â€” | **+430ï¼ˆV1.0: 830 â†’ V1.1: 1260ï¼‰** |
