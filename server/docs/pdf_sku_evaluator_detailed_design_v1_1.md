# Evaluator æ¨¡å—è¯¦ç»†è®¾è®¡

> **æ–‡æ¡£ç‰ˆæœ¬**: V1.1  
> **ä¸Šæ¸¸ä¾èµ–**: TA V1.6 Â§3.2 | BRD V2.1 Â§12 | BA V1.1 Â§2.2  
> **æ¨¡å—å®šä½**: æ–‡æ¡£è´¨é‡è¯„ä¼° + è·¯ç”±å†³ç­– â€” å†³å®š PDF èµ° AUTO / HYBRID / HUMAN_ALL  
> **è®¾è®¡åŸåˆ™**: è·¯ç”±å‡†ç¡®ç‡æœ€ä¼˜å…ˆã€ç¼“å­˜é˜²ç©¿é€ã€è¯„ä¼°å¯è§£é‡Š

### V1.1 ä¿®è®¢è¯´æ˜

| å˜æ›´ ID | çº§åˆ« | è¯´æ˜ | æ¥æº |
|---------|------|------|------|
| P0-3 | P0 | è¯„ä¼°å¤±è´¥é™çº§ç­–ç•¥ï¼šLLM API å¼‚å¸¸â†’HUMAN_ALL | Qwen3 |
| P0-4 | P0 | è¡¥å…… evaluate_page_lightweight å®Œæ•´å®šä¹‰ | Kimi |
| P0-5 | P0 | all_blank é¢„ç­›ç›´æ¥é™çº§ï¼ˆå¯¹é½ BA BR-25ï¼‰ | Qwen3 |
| P1-E1 | P1 | é‡‡æ ·ç‰¹å¾åŠ æƒï¼šæŒ‰å›¾ç‰‡å¯†åº¦/OCRç‡åˆ†å±‚ | Gemini |
| P1-E2 | P1 | Redis é”ç»­çº¦ï¼ˆextend_lock æ¯ 30sï¼‰ | Gemini |
| P1-E3 | P1 | æ–¹å·®+ç†µå€¼åŒé‡æ£€æŸ¥å¼ºåˆ¶ HYBRID | Gemini |
| P1-E4 | P1 | æ•…éšœæ¢å¤å¼ºåˆ¶è¯» DB | Qwen3 |
| P1-E5 | P1 | é‡‡æ ·è¿‡æ»¤ç›®å½•é¡µ | Kimi |
| P1-E6 | P1 | ç¼“å­˜é”®å¢åŠ  revision | Kimi |
| P1-E7 | P1 | Prompt ç‰ˆæœ¬å­˜å…¥ Evaluation æŠ¥å‘Š | DeepSeek |
| P1-E8 | P1 | Job çº§è¿ç»­ Fallback ç†”æ–­ä¿¡å· | Gemini |
| P1-E9 | P1 | all_blank ç›´æ¥é™çº§é€»è¾‘ï¼ˆä»£ç å±‚ï¼‰ | Qwen3 |

---

## 1. æ¨¡å—èŒè´£è¾¹ç•Œ

### 1.1 èŒè´£èŒƒå›´

| èŒè´£ | è¯´æ˜ | å¯¹é½ |
|------|------|------|
| **é‡‡æ ·ç­–ç•¥** | â‰¤40 é¡µå…¨é‡ï¼Œ>40 é¡µæŒ‰ç­–ç•¥æŠ½æ ·ï¼ˆå«ç©ºç™½é¡µæ’é™¤ï¼‰ | TA Â§3.2 |
| **å¤šç»´è¯„åˆ†** | LLM è¯„ä¼°é‡‡æ ·é¡µ â†’ ç»´åº¦å¾—åˆ† â†’ åŠ æƒèšåˆ â†’ C_doc | TA Â§3.2 |
| **è·¯ç”±å†³ç­–** | C_doc ä¸ {A, B, PV} é˜ˆå€¼æ¯”è¾ƒ + æ–¹å·®å¼ºåˆ¶ HYBRID | TA Â§3.2 |
| **åŒå±‚ç¼“å­˜** | file_hash + config_version â†’ Redis(å¿«) + DB(æŒä¹…) | TA Â§3.2 |
| **åˆ†å¸ƒå¼é”** | ç›¸åŒ file_hash å¹¶å‘è¯„ä¼°é˜²ç©¿é€ | TA Â§3.2 |
| **HYBRID é¡µçº§è¯„ä¼°** | å¤ç”¨æ–‡æ¡£çº§é‡‡æ · + Qwen ä½æˆæœ¬è¡¥å…… | V1.6:P1-5/P1-7 |
| **è¯„ä¼°æŠ¥å‘Š** | å¯è§£é‡Šç»“æ„åŒ–è¾“å‡ºï¼ˆé¢„ç­›/ç»´åº¦/è·¯ç”±/ç†ç”±ï¼‰ | V1.6:P1-11 |
| **é¢„ç­›è§„åˆ™é…ç½®åŒ–** | Config æ¨¡å—çƒ­æ›´æ–°çš„ PrescanRuleConfig | V1.6:P1-6 |

### 1.2 ä¸è´Ÿè´£çš„äº‹

| ä¸åš | å½’å± |
|------|------|
| é¢„ç­›æ‰§è¡Œ | Gatewayï¼ˆPrescannerï¼‰ |
| LLM è°ƒç”¨/ç†”æ–­ | LLM Adapter |
| é¡µé¢å¤„ç† | Pipeline |

### 1.3 ä¾èµ–

```mermaid
graph LR
    GW[Gateway] -->|"JobCreated + PrescanResult"| EV[Evaluator]
    EV --> LLM[LLM Adapter]
    EV --> Config[ConfigProvider]
    EV --> Redis[(Redis<br/>ç¼“å­˜+é”)]
    EV --> DB[(PostgreSQL<br/>evaluations)]
    EV -->|"route decided"| PL[Pipeline]
    style EV fill:#1a1f2c,stroke:#FFA500,color:#E2E8F4
```

---

## 2. ç›®å½•ç»“æ„

```
app/
â”œâ”€â”€ evaluator/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ evaluator.py            # ä¸»å…¥å£ï¼šç¼“å­˜â†’é‡‡æ ·â†’è¯„åˆ†â†’è·¯ç”±
â”‚   â”œâ”€â”€ sampler.py              # é‡‡æ ·ç­–ç•¥ï¼ˆç‰¹å¾åŠ æƒ+ç›®å½•é¡µè¿‡æ»¤ï¼‰  [V1.1]
â”‚   â”œâ”€â”€ scorer.py               # å¤šç»´åº¦è¯„åˆ† + åŠ æƒèšåˆ
â”‚   â”œâ”€â”€ router.py               # è·¯ç”±å†³ç­–ï¼ˆA/B/PV é˜ˆå€¼ï¼‰+ é™çº§å®ˆå«  [V1.1]
â”‚   â”œâ”€â”€ cache.py                # åŒå±‚ç¼“å­˜ï¼ˆRedis+DBï¼‰+ åˆ†å¸ƒå¼é” + ç»­çº¦  [V1.1]
â”‚   â”œâ”€â”€ variance_detector.py    # é‡‡æ ·å¾—åˆ†æ–¹å·®+ç†µå€¼æ£€æµ‹  [V1.1]
â”‚   â”œâ”€â”€ hybrid_evaluator.py     # HYBRID é¡µçº§è¯„ä¼°ï¼ˆå¤ç”¨+Qwenè¡¥å……ï¼‰
â”‚   â”œâ”€â”€ eval_report.py          # è¯„ä¼°æŠ¥å‘Šå¯è§£é‡Šç»“æ„åŒ–
â”‚   â”œâ”€â”€ fallback_monitor.py     # [V1.1] Job çº§è¿ç»­ Fallback ç†”æ–­ä¿¡å·
â”‚   â”œâ”€â”€ schemas.py              # Evaluation, Score, SamplingInfo
â”‚   â”œâ”€â”€ repository.py           # evaluations è¡¨ CRUD
â”‚   â””â”€â”€ constants.py
```

---

## 3. ç±»å›¾

```mermaid
classDiagram
    class EvaluatorService {
        -cache: EvalCache
        -sampler: Sampler
        -scorer: Scorer
        -router: RouteDecider
        -llm: LLMService
        -prompt: PromptEngine
        -variance: VarianceDetector
        -repo: EvalRepository
        -fallback_monitor: FallbackMonitor
        +evaluate(job, prescan) Evaluation
        -_evaluate_inner(job, prescan, profile) Evaluation
    }

    class EvalCache {
        -redis: Redis
        -db: EvalRepository
        +get(file_hash, config_version, revision?) Evaluation?
        +get_from_db(file_hash, config_version) Evaluation?
        +put(evaluation) void
        +lock(cache_key, timeout) AsyncContextManager
        -_extend_lock_loop(lock_key, interval) void
    }

    class Sampler {
        +FULL_THRESHOLD: 40
        +select_pages(total, blanks, threshold, page_features?) list~int~
        -_stratified_sample(total, blanks) list~int~
        -_feature_weighted_sample(effective, page_features, sample_size) list~int~
        -_is_toc_page(page_no, features) bool
    }

    class Scorer {
        +DIMENSIONS: list~str~
        +aggregate(llm_scores) dict
        +compute_c_doc(dimensions, weights, penalty) float
    }

    class RouteDecider {
        +decide(c_doc, thresholds, variance_forced, prescan?) tuple
        -_build_reason(c_doc, thresholds, route, extra?) str
        -_check_prescan_guard(prescan) tuple?
    }

    class VarianceDetector {
        +check(page_scores, threshold?) tuple~float_bool~
        -_compute_entropy(scores) float
    }

    class HybridEvaluator {
        +evaluate_pages(job, evaluation) dict
        +evaluate_page_lightweight(file_path, page_no, profile) float
    }

    class FallbackMonitor {
        +CONSECUTIVE_THRESHOLD: int
        +on_page_fallback(job_id, page_no) void
        +should_suspend(job_id) bool
        +reset(job_id) void
    }

    class EvalReport {
        +build(evaluation, prescan) EvaluationReport
    }

    EvaluatorService --> EvalCache
    EvaluatorService --> Sampler
    EvaluatorService --> Scorer
    EvaluatorService --> RouteDecider
    EvaluatorService --> VarianceDetector
    EvaluatorService --> HybridEvaluator
    EvaluatorService --> FallbackMonitor
    EvaluatorService --> EvalReport
```

---

## 4. æ ¸å¿ƒæ—¶åºå›¾

```mermaid
sequenceDiagram
    autonumber
    participant GW as Gateway
    participant ES as EvaluatorService
    participant Cache as EvalCache
    participant Redis
    participant DB as PostgreSQL
    participant Pool as ProcessPool
    participant LLM as LLM Adapter
    participant Router as RouteDecider

    GW->>+ES: evaluate(job, prescan)

    rect rgb(25, 30, 45)
    Note over ES: [V1.1 P0-5] Step 0: Prescan Guard â€” all_blank ç›´æ¥é™çº§
    alt prescan.all_blank == true
        ES->>DB: INSERT INTO evaluations (route=HUMAN_ALL, degrade_reason=prescan_reject)
        ES-->>GW: Evaluation{route=HUMAN_ALL, degrade_reason="prescan_reject"}
    end
    end

    rect rgb(30, 40, 60)
    Note over ES,DB: Step 1: åŒå±‚ç¼“å­˜æŸ¥è¯¢
    ES->>+Cache: get(file_hash, config_version)
    Cache->>Redis: GET eval:{hash}:{ver}
    alt Redis å‘½ä¸­
        Redis-->>Cache: cached
        Cache-->>ES: Evaluation
        ES-->>GW: Evaluation (cached)
    else Redis miss
        Cache->>DB: SELECT FROM evaluations WHERE hash+ver
        alt DB å‘½ä¸­
            DB-->>Cache: row
            Cache->>Redis: SETEX (TTL å¯é…ç½®ï¼Œé»˜è®¤ 24h)
            Cache-->>ES: Evaluation
        else DB miss
            Cache-->>-ES: None
        end
    end
    end

    rect rgb(40, 30, 50)
    Note over ES,LLM: Step 2: åˆ†å¸ƒå¼é”å†…è¯„ä¼° [V1.1] é”ç»­çº¦
    ES->>Redis: SETNX lock:eval:{hash}:{ver} (300s TTL)
    Note over ES: [V1.1 P1-E2] å¯åŠ¨åå°ç»­çº¦åç¨‹(æ¯30s PEXPIRE)
    Note over ES: Double-check after lock
    ES->>Redis: GET eval:{hash}:{ver}
    alt é”å†…å†æ¬¡å‘½ä¸­
        ES-->>GW: Evaluation (race winner)
    end

    Note over ES: [V1.1 P1-E1/E5] ç‰¹å¾åŠ æƒé‡‡æ · + ç›®å½•é¡µè¿‡æ»¤
    ES->>ES: sampler.select_pages(total, blanks, 40, page_features)

    ES->>+Pool: render_pages_batch(file, sample_pages)
    Pool-->>-ES: screenshots[]

    rect rgb(50, 30, 30)
    Note over ES,LLM: [V1.1 P0-3] LLM è°ƒç”¨ + å¤±è´¥é™çº§
    ES->>+LLM: evaluate_document(screenshots, features, prompt)
    alt LLM æˆåŠŸ
        LLM-->>-ES: Score[]
    else LLM è¶…æ—¶/ç†”æ–­/å¼‚å¸¸
        Note over ES: æ•è·å¼‚å¸¸ â†’ route=HUMAN_ALL,<br/>degrade_reason=eval_failed
        ES->>DB: INSERT evaluations (route=HUMAN_ALL, degrade_reason)
        ES-->>GW: Evaluation{route=HUMAN_ALL, degrade_reason="eval_failed"}
    end
    end

    ES->>ES: scorer.aggregate(scores) â†’ dimension_scores
    ES->>ES: scorer.compute_c_doc(dims, weights, penalty)
    Note over ES: [V1.1 P1-E3] æ–¹å·®+ç†µå€¼åŒé‡æ£€æŸ¥
    ES->>ES: variance_detector.check(page_scores)
    ES->>+Router: decide(c_doc, thresholds, variance_forced)
    Router-->>-ES: (route, degrade_reason)
    end

    rect rgb(30, 50, 40)
    Note over ES,Redis: Step 3: å…ˆå†™ DB å†å†™ Redis [V1.1 P1-E7] å« prompt_version
    ES->>DB: INSERT INTO evaluations (å« prompt_version)
    ES->>Redis: SETEX eval:{hash}:{ver} TTL
    ES->>Redis: DEL lock:eval:{hash}:{ver}
    end

    ES-->>-GW: Evaluation{route=HYBRID, c_doc=0.72}
```

---

## 5. ç»„ä»¶è¯¦ç»†è§„æ ¼

### 5.1 EvaluatorService â€” ä¸»å…¥å£

```python
class EvaluatorService:
    """
    [V1.1] å˜æ›´ï¼š
    - P0-3: LLM å¼‚å¸¸æ—¶é™çº§ HUMAN_ALLï¼ˆä¸å†æŠ›å¼‚å¸¸å¡æ­» Jobï¼‰
    - P0-5: all_blank é¢„ç­›ç›´æ¥é™çº§ï¼ˆå¯¹é½ BA BR-25ï¼‰
    - P1-E2: åˆ†å¸ƒå¼é”ç»­çº¦ï¼ˆåå°åç¨‹æ¯ 30s PEXPIREï¼‰
    - P1-E4: æ•…éšœæ¢å¤æ—¶ get_from_db å¼ºåˆ¶è¯» DB
    - P1-E7: prompt_version å­˜å…¥ Evaluation
    """

    async def evaluate(self, job: PDFJob, prescan: PrescanResult) -> Evaluation:
        profile = config.get_profile(job.frozen_config_version)

        # [V1.1 P0-5] Prescan Guard: all_blank ç›´æ¥é™çº§
        if prescan.all_blank:
            return await self._create_degraded_evaluation(
                job, prescan, profile,
                route="HUMAN_ALL", degrade_reason="prescan_reject")

        # [V1.1 P1-E6] ç¼“å­˜é”®åŒ…å« revisionï¼ˆåŒæ–‡ä»¶ä¸åŒä¿®è®¢ç‰ˆé‡æ–°è¯„ä¼°ï¼‰
        cache_key = f"{job.file_hash}:{job.frozen_config_version}"
        if job.file_revision:
            cache_key += f":{job.file_revision}"

        # 1. ç¼“å­˜æŸ¥è¯¢
        cached = await self._cache.get(cache_key)
        if cached:
            return cached

        # 2. åˆ†å¸ƒå¼é”å†…è¯„ä¼° [V1.1 P1-E2 é”ç»­çº¦]
        async with self._cache.lock(cache_key, timeout=300) as lock_ctx:
            cached = await self._cache.get(cache_key)
            if cached:
                return cached

            try:
                return await self._evaluate_inner(job, prescan, profile, cache_key)
            except (LLMCircuitOpenError, LLMTimeoutError, Exception) as e:
                # [V1.1 P0-3] è¯„ä¼°å¤±è´¥é™çº§ï¼šä¸è®© Job å¡æ­»
                logger.error("eval_failed_degrade",
                    job_id=str(job.job_id), error=str(e))
                metrics.eval_degrade_total.labels(reason="eval_failed").inc()
                return await self._create_degraded_evaluation(
                    job, prescan, profile,
                    route="HUMAN_ALL", degrade_reason=f"eval_failed:{type(e).__name__}")

    async def _evaluate_inner(
        self, job: PDFJob, prescan: PrescanResult,
        profile: ThresholdProfile, cache_key: str
    ) -> Evaluation:
        # 3. é‡‡æ ·+æˆªå›¾ [V1.1 P1-E1 ç‰¹å¾åŠ æƒ + P1-E5 ç›®å½•é¡µè¿‡æ»¤]
        page_features = prescan.page_features if hasattr(prescan, 'page_features') else None
        sample_pages = self._sampler.select_pages(
            job.total_pages, prescan.blank_pages,
            threshold=40, page_features=page_features)
        screenshots = await asyncio.get_event_loop().run_in_executor(
            process_pool, render_pages_batch, job.file_path, sample_pages)
        features = [extract_quick_features(job.file_path, p) for p in sample_pages]

        # 4. LLM è¯„ä¼°ï¼ˆå¼‚å¸¸ç”±ä¸Šå±‚ P0-3 æ•è·ï¼‰
        llm_scores = await self._llm.evaluate_document(
            screenshots=screenshots, features=features,
            prompt=self._prompt.get_eval_prompt(profile))

        # 5. èšåˆ+è·¯ç”± [V1.1 P1-E3 æ–¹å·®+ç†µå€¼]
        dimension_scores = self._scorer.aggregate(llm_scores)
        c_doc = self._scorer.compute_c_doc(
            dimension_scores, profile.weights, prescan.total_penalty)
        page_scores = [s.overall for s in llm_scores]
        variance, variance_forced = self._variance.check(
            page_scores,
            threshold=profile.prescan_rules.get("score_variance_threshold"))
        route, degrade_reason = self._router.decide(
            c_doc, profile.thresholds, variance_forced)

        # 6. æ„å»º+æŒä¹…åŒ– [V1.1 P1-E7 prompt_version]
        evaluation = Evaluation(
            file_hash=job.file_hash,
            config_version=job.frozen_config_version,
            file_revision=getattr(job, 'file_revision', None),
            doc_confidence=c_doc, route=route,
            degrade_reason=degrade_reason,
            dimension_scores=dimension_scores,
            weights_snapshot=profile.weights,
            sampling=SamplingInfo(pages=sample_pages, variance=variance),
            prescan=prescan,
            model_used=self._llm.current_model_name,
            prompt_version=self._prompt.get_version("eval_document"),
            page_evaluations={str(p): s.overall
                for p, s in zip(sample_pages, llm_scores)},
        )
        await self._repo.insert(evaluation)
        await self._cache.put(cache_key, evaluation)
        return evaluation

    async def _create_degraded_evaluation(
        self, job, prescan, profile, route, degrade_reason
    ) -> Evaluation:
        """åˆ›å»ºé™çº§è¯„ä¼°ç»“æœï¼ˆP0-3/P0-5 å…±ç”¨ï¼‰"""
        evaluation = Evaluation(
            file_hash=job.file_hash,
            config_version=job.frozen_config_version,
            doc_confidence=0.0, route=route,
            degrade_reason=degrade_reason,
            dimension_scores={},
            weights_snapshot=profile.weights,
            sampling=SamplingInfo(pages=[], variance=0.0),
            prescan=prescan,
            model_used=None,
            prompt_version=None,
            page_evaluations={},
        )
        await self._repo.insert(evaluation)
        return evaluation
```

### 5.2 RouteDecider â€” è·¯ç”±å†³ç­–

```python
class RouteDecider:
    """
    è·¯ç”±å†³ç­–çŸ©é˜µï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ æ¡ä»¶                         â”‚ è·¯ç”±      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ c_doc â‰¥ A(0.90)             â”‚ AUTO     â”‚
    â”‚ B(0.40) â‰¤ c_doc < A        â”‚ HYBRID   â”‚
    â”‚ c_doc < B(0.40)             â”‚ HUMAN_ALLâ”‚
    â”‚ variance_forced = true      â”‚ HYBRID   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    ä¸å˜å¼ INV-01: B < PV < A (ç”± Config æ¨¡å—ä¿è¯)
    """

    def decide(
        self, c_doc: float, thresholds: dict, variance_forced: bool
    ) -> tuple[str, str | None]:
        A = thresholds.get("A", 0.90)
        B = thresholds.get("B", 0.40)

        if variance_forced and c_doc >= B:
            route = "HYBRID"
            reason = self._build_reason(c_doc, thresholds, route,
                                         extra="variance_forced")
            return route, reason

        if c_doc >= A:
            return "AUTO", None
        elif c_doc >= B:
            return "HYBRID", self._build_reason(c_doc, thresholds, "HYBRID")
        else:
            return "HUMAN_ALL", self._build_reason(c_doc, thresholds, "HUMAN_ALL")

    def _build_reason(self, c_doc, thresholds, route, extra=None) -> str:
        parts = [f"C_doc={c_doc:.3f}"]
        parts.append(f"A={thresholds.get('A', 0.90)}")
        parts.append(f"B={thresholds.get('B', 0.40)}")
        parts.append(f"â†’ {route}")
        if extra:
            parts.append(f"({extra})")
        return ", ".join(parts)
```

### 5.3 Sampler â€” é‡‡æ ·ç­–ç•¥

```python
class Sampler:
    """
    [V1.1] å˜æ›´ï¼š
    - P1-E1: ç‰¹å¾åŠ æƒé‡‡æ ·ï¼ˆæŒ‰å›¾ç‰‡å¯†åº¦/OCRç‡åˆ†å±‚ï¼‰ï¼Œé¿å…å¤æ‚é¡µé›†ä¸­åœ¨æœªé‡‡æ ·åŒº
    - P1-E5: è¿‡æ»¤ç›®å½•é¡µï¼ˆç›®å½•é¡µè¢«é‡‡ä¸­ä¼šæ‹‰ä½è¯„åˆ†å¯¼è‡´è¯¯åˆ¤ HUMAN_ALLï¼‰
    """

    FULL_THRESHOLD = 40
    TOC_KEYWORDS = {"ç›®å½•", "contents", "table of contents", "index"}

    def select_pages(
        self, total: int, blank_pages: list[int],
        threshold: int = None, page_features: dict | None = None,
    ) -> list[int]:
        threshold = threshold or self.FULL_THRESHOLD
        effective = [p for p in range(1, total + 1) if p not in (blank_pages or [])]

        # [V1.1 P1-E5] è¿‡æ»¤ç›®å½•é¡µ
        if page_features:
            effective = [p for p in effective
                         if not self._is_toc_page(p, page_features)]

        if len(effective) <= threshold:
            return effective

        # [V1.1 P1-E1] ç‰¹å¾åŠ æƒé‡‡æ ·
        if page_features:
            return self._feature_weighted_sample(effective, page_features, threshold)

        # Fallback: åŸå§‹åˆ†å±‚æŠ½æ ·
        return self._stratified_sample(effective, threshold)

    def _feature_weighted_sample(
        self, effective: list[int], page_features: dict, sample_size: int
    ) -> list[int]:
        """
        æŒ‰ç‰¹å¾å¤æ‚åº¦åˆ†å±‚ï¼š
        - é«˜å¤æ‚åº¦ï¼ˆå›¾ç‰‡å¯†åº¦ > 5 æˆ– OCR ç‡ < 0.5ï¼‰ï¼šæƒé‡ 3
        - ä¸­å¤æ‚åº¦ï¼šæƒé‡ 2
        - ä½å¤æ‚åº¦ï¼šæƒé‡ 1
        å„å±‚æŒ‰æ¯”ä¾‹æŠ½æ ·ï¼Œç¡®ä¿é¦–å°¾å„ 2 é¡µå¿…é€‰ã€‚
        """
        head = effective[:2]
        tail = effective[-2:]
        middle = [p for p in effective if p not in head and p not in tail]

        high, med, low = [], [], []
        for p in middle:
            feat = page_features.get(p, {})
            img_density = feat.get("image_count", 0)
            ocr_rate = feat.get("ocr_rate", 1.0)
            if img_density > 5 or ocr_rate < 0.5:
                high.append(p)
            elif img_density > 2 or ocr_rate < 0.8:
                med.append(p)
            else:
                low.append(p)

        remaining = sample_size - len(head) - len(tail)
        total_weight = len(high) * 3 + len(med) * 2 + len(low) * 1
        if total_weight == 0:
            return sorted(set(head + tail))

        def pick(pool, weight):
            n = max(1, int(remaining * len(pool) * weight / total_weight))
            return random.sample(pool, min(n, len(pool)))

        selected = head + tail + pick(high, 3) + pick(med, 2) + pick(low, 1)
        return sorted(set(selected))[:sample_size]

    def _stratified_sample(self, effective: list[int], sample_size: int) -> list[int]:
        """åŸå§‹åˆ†å±‚æŠ½æ ·ï¼ˆé¦–å°¾å„ 2 é¡µ + ä¸­é—´å‡åŒ€ï¼‰"""
        head = effective[:2]
        tail = effective[-2:]
        middle_pool = effective[2:-2]
        remaining = sample_size - len(head) - len(tail)
        if remaining > 0 and middle_pool:
            step = max(1, len(middle_pool) // remaining)
            middle = middle_pool[::step][:remaining]
        else:
            middle = []
        return sorted(set(head + middle + tail))

    def _is_toc_page(self, page_no: int, page_features: dict) -> bool:
        """æ£€æµ‹ç›®å½•é¡µï¼šæ–‡å­—å«ç›®å½•å…³é”®è¯ + å›¾ç‰‡æ•° = 0"""
        feat = page_features.get(page_no, {})
        text_hint = feat.get("text_hint", "").lower()
        return (feat.get("image_count", 0) == 0 and
                any(kw in text_hint for kw in self.TOC_KEYWORDS))
```

### 5.4 HybridEvaluator â€” HYBRID é¡µçº§è¯„ä¼°

```python
class HybridEvaluator:
    """
    HYBRID æ¨¡å¼é¡µçº§è¯„ä¼°ï¼ˆV1.6:P1-5/P1-7ï¼‰ï¼š
    - å·²é‡‡æ ·é¡µï¼šå¤ç”¨æ–‡æ¡£çº§å¾—åˆ†ï¼ˆé›¶ LLM æˆæœ¬ï¼‰
    - æœªé‡‡æ ·é¡µï¼šQwen ä½æˆæœ¬æ¨¡å‹è¯„ä¼°ï¼ˆGemini 40% æˆæœ¬ï¼‰
    """

    async def evaluate_pages(
        self, job: PDFJob, evaluation: Evaluation
    ) -> dict[int, float]:
        sampled_scores = evaluation.page_evaluations or {}
        profile = config.get_profile(job.frozen_config_version)
        results = {}

        for page_no in range(1, job.total_pages + 1):
            if page_no in (evaluation.prescan.blank_pages or []):
                continue
            if str(page_no) in sampled_scores:
                results[page_no] = sampled_scores[str(page_no)]
            else:
                score = await self.evaluate_page_lightweight(
                    job.file_path, page_no, profile)
                results[page_no] = score

        return results

    async def evaluate_page_lightweight(
        self, file_path: str, page_no: int, profile: ThresholdProfile
    ) -> float:
        """
        [V1.1 P0-4] è½»é‡é¡µçº§è¯„ä¼° â€” å®Œæ•´å®šä¹‰

        æ¨¡å‹ï¼šQwen æ ‡å‡†æ¨¡å‹ï¼ˆæˆæœ¬çº¦ Gemini çš„ 40%ï¼‰
        è¾“å…¥ï¼šå•é¡µæˆªå›¾ 150 DPI + å¿«é€Ÿç‰¹å¾ï¼ˆOCR ç‡ã€å›¾ç‰‡æ•°ã€æ–‡æœ¬å¯†åº¦ï¼‰
        è¾“å‡ºï¼š0.0~1.0 ç½®ä¿¡åº¦åˆ†æ•°
        è¶…æ—¶ï¼šå•é¡µ 30s
        é”™è¯¯ï¼šè¶…æ—¶/å¼‚å¸¸ â†’ è¿”å› 0.5ï¼ˆä¸­æ€§åˆ†ï¼Œä¸å½±å“è·¯ç”±å€¾å‘ï¼‰

        Prompt æ¨¡æ¿ï¼š
        ---
        You are evaluating a single page from a product catalog PDF.
        Rate the page's suitability for automated SKU extraction on a 0.0-1.0 scale.
        Consider: text clarity, image quality, layout structure, table regularity.
        Respond with ONLY a JSON: {"score": 0.XX, "reason": "brief explanation"}
        ---
        """
        try:
            screenshot = await asyncio.get_event_loop().run_in_executor(
                process_pool, render_single_page, file_path, page_no, 150)  # 150 DPI
            features = extract_quick_features(file_path, page_no)

            result = await asyncio.wait_for(
                self._llm.evaluate_single_page(
                    screenshot=screenshot,
                    features=features,
                    prompt=self._prompt.get_eval_prompt(profile, variant="lightweight"),
                    model_override="QWEN_STANDARD",
                ),
                timeout=30.0,
            )
            return result.score
        except (asyncio.TimeoutError, Exception) as e:
            logger.warning("lightweight_eval_failed",
                page_no=page_no, error=str(e))
            return 0.5  # ä¸­æ€§åˆ†ï¼šä¸ä¼šå€¾å‘ AUTO ä¹Ÿä¸ä¼šå€¾å‘ HUMAN
```

### 5.5 FallbackMonitor â€” Job çº§è¿ç»­ Fallback ç†”æ–­

```python
class FallbackMonitor:
    """
    [V1.1 P1-E8] Job çº§è¿ç»­ Fallback ç†”æ–­ä¿¡å·

    Pipeline è¿ç»­ N é¡µè§¦å‘ Fallback â†’ æš‚åœ Job â†’ å‰©ä½™é¡µå¼ºåˆ¶ HUMAN_PAGEã€‚
    ç”± Pipeline Orchestrator è°ƒç”¨ on_page_fallback()ï¼Œé€šè¿‡ should_suspend() æŸ¥è¯¢ã€‚
    """

    CONSECUTIVE_THRESHOLD = 3  # è¿ç»­ 3 é¡µ Fallback å³è§¦å‘

    def __init__(self):
        self._counters: dict[str, int] = {}  # job_id â†’ consecutive count

    def on_page_fallback(self, job_id: str, page_no: int):
        """Pipeline æ¯æ¬¡é¡µé¢ Fallback æ—¶è°ƒç”¨"""
        self._counters[job_id] = self._counters.get(job_id, 0) + 1
        if self._counters[job_id] >= self.CONSECUTIVE_THRESHOLD:
            logger.warning("job_fallback_threshold_reached",
                job_id=job_id, consecutive=self._counters[job_id])
            metrics.eval_job_suspended_total.inc()

    def on_page_success(self, job_id: str):
        """Pipeline é¡µé¢æˆåŠŸå¤„ç†æ—¶é‡ç½®è®¡æ•°"""
        self._counters[job_id] = 0

    def should_suspend(self, job_id: str) -> bool:
        return self._counters.get(job_id, 0) >= self.CONSECUTIVE_THRESHOLD

    def reset(self, job_id: str):
        self._counters.pop(job_id, None)
```

---

## 6. é”™è¯¯å¤„ç†

| åœºæ™¯ | å¤„ç† | é™çº§ |
|------|------|------|
| [V1.1 P0-5] all_blank é¢„ç­› | ç›´æ¥é™çº§ï¼Œä¸è¿›å…¥ LLM è¯„ä¼° | route=HUMAN_ALL, reason=prescan_reject |
| [V1.1 P0-3] LLM è¯„ä¼°è¶…æ—¶/å¼‚å¸¸ | æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œåˆ›å»ºé™çº§ Evaluation | route=HUMAN_ALL, reason=eval_failed:{ExceptionType} |
| LLM å…¨ç†”æ–­ | æ•è· LLMCircuitOpenError | route=HUMAN_ALL, reason=eval_failed:LLMCircuitOpenError |
| Redis ç¼“å­˜ä¸å¯ç”¨ | è·³è¿‡ç¼“å­˜ç›´æ¥ DB | æ—¥å¿— WARNING |
| [V1.1 P1-E4] æ•…éšœæ¢å¤ | get_from_db å¼ºåˆ¶è¯» DBï¼Œå¿½ç•¥ Redis | ç¡®ä¿æ•°æ®ä¸€è‡´æ€§ |
| åˆ†å¸ƒå¼é”è¶…æ—¶ | [V1.1 P1-E2] ç»­çº¦åç¨‹é¿å…æå‰é‡Šæ”¾ | ç»­çº¦å¤±è´¥åˆ™é‡Šæ”¾é”ï¼Œé‡è¯•ä¸€æ¬¡ |
| é‡‡æ ·é¡µè¿‡å°‘(<3) | å…¨é‡è¯„ä¼° | â€” |
| [V1.1 P1-E8] è¿ç»­ Fallback | Pipeline è°ƒç”¨ FallbackMonitor | é€šçŸ¥ Orchestrator æŒ‚èµ· Job |

---

## 7. Prometheus æŒ‡æ ‡

```python
eval_cache_total = Counter("eval_cache_total", "Cache hits/misses", ["result"])
eval_duration_seconds = Histogram("eval_duration_seconds", "Evaluation time")
eval_route_total = Counter("eval_route_total", "Route decisions", ["route"])
eval_c_doc = Histogram("eval_c_doc", "C_doc distribution", buckets=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0])
eval_variance_forced_total = Counter("eval_variance_forced_total", "Variance-forced HYBRID")
# [V1.1] æ–°å¢
eval_degrade_total = Counter("eval_degrade_total", "Degraded evaluations", ["reason"])
eval_lock_renewed_total = Counter("eval_lock_renewed_total", "Lock renewal count")
eval_toc_filtered_total = Counter("eval_toc_filtered_total", "TOC pages filtered from sampling")
eval_job_suspended_total = Counter("eval_job_suspended_total", "Jobs suspended by FallbackMonitor")
eval_lightweight_duration = Histogram("eval_lightweight_duration", "Lightweight page eval time")
```

---

## 8. äº¤ä»˜æ¸…å•

| æ–‡ä»¶ | è¡Œæ•°(ä¼°) | ä¼˜å…ˆçº§ | V1.1 å˜æ›´ |
|------|---------|--------|----------|
| `evaluator.py` | ~280 | P0 | +80: é™çº§é€»è¾‘ / prescan guard / cache_key revision |
| `sampler.py` | ~150 | P0 | +70: ç‰¹å¾åŠ æƒé‡‡æ · / ç›®å½•é¡µè¿‡æ»¤ |
| `scorer.py` | ~100 | P0 | â€” |
| `router.py` | ~80 | P0 | â€” |
| `cache.py` | ~160 | P0 | +40: é”ç»­çº¦ / get_from_db / TTL å¯é…ç½® |
| `variance_detector.py` | ~60 | P0 | +20: ç†µå€¼è®¡ç®— / é˜ˆå€¼é…ç½®åŒ– |
| `hybrid_evaluator.py` | ~140 | P1 | +60: evaluate_page_lightweight å®Œæ•´å®ç° |
| `fallback_monitor.py` | ~50 | P1 | ğŸ†• æ–°å¢ |
| `eval_report.py` | ~80 | P1 | +20: prompt_version å­—æ®µ |
| `schemas.py` | ~100 | P0 | +20: file_revision / degrade fields |
| `repository.py` | ~80 | P0 | â€” |
| `constants.py` | ~40 | P0 | +10: æ–°å¢é…ç½®é¡¹ |
| **æ€»è®¡** | **~1320** | â€” | **+370ï¼ˆV1.0: 950 â†’ V1.1: 1320ï¼‰** |
